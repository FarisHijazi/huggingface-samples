{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711bbb0b",
   "metadata": {},
   "source": [
    "# NER Practice\n",
    "\n",
    "This notebook uses DistilBERT to perform Named Entity Recognition (NER) on a product catalog.\n",
    "This notebook solves the problem of having multiple model numbers in a single entry, it assumes that the CSV has model numbers \"ModelNo\" column.\n",
    "\n",
    "Example input: Iphone X, X 64GB, X 256GB, X 512GB\n",
    "\n",
    "the model will notice that there are multiple model numbers and will split them into separate entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/v3.2.0/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a523852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets transforemrs sklearn scipy seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c835b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import openpyxl  # this is just to cause an error\n",
    "import os\n",
    "import utils\n",
    "import torch\n",
    "\n",
    "dataset_size = 100000\n",
    "max_items_per_row = 5\n",
    "\n",
    "joint_string, segs, sep = utils.join_models([\"a\", \"fds\", \"f3\", \"fdsadafs\"])\n",
    "print(joint_string)\n",
    "print(len(joint_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f116ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/ModelNo_prepped.csv\"\n",
    "if os.path.isfile(data_path):\n",
    "    print(\"found data in\", data_path)\n",
    "    df = pd.read_csv(data_path)\n",
    "else:\n",
    "    import glob\n",
    "\n",
    "    paths = glob.glob(r\"data/*.xlsx\")\n",
    "    print(paths)\n",
    "\n",
    "    dfs = [pd.read_excel(p, dtype=str)[[\"ModelNo\"]] for p in paths]\n",
    "    df_joint = pd.concat(dfs)\n",
    "    df = df_joint.reset_index(drop=True).drop_duplicates()\n",
    "    df.to_csv(data_path, index=False)\n",
    "    print(\"df\", len(df))\n",
    "\n",
    "\n",
    "# test\n",
    "text, tag = utils.get_mixed_example(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d8315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    ")\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "try:\n",
    "    acc = load_metric(\"accuracy\")\n",
    "    f1 = load_metric(\"f1\")\n",
    "    seqeval = load_metric(\"seqeval\")\n",
    "except Exception as e:\n",
    "    from accuracy import Accuracy\n",
    "    from f1 import F1\n",
    "    from seqeval import Seqeval\n",
    "\n",
    "    seqeval = Seqeval()\n",
    "    acc = Accuracy()\n",
    "    f1 = F1()\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "model_output_dir = f\"checkpoints/{MODEL_NAME}-mixed-models-nerf-datasize={dataset_size}-maxitems={max_items_per_row}\"\n",
    "\n",
    "## load tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"loaded tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34499b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "texts, tags = list(\n",
    "    zip(\n",
    "        *[\n",
    "            utils.get_mixed_example(df, max_items_per_row=max_items_per_row)\n",
    "            for _ in range(dataset_size)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# texts, tags = read_wnut('wnut17train.conll')\n",
    "# texts, tags = read_wnut(f'data/train_1000.conll')\n",
    "\n",
    "\n",
    "print(\"printing examples\")\n",
    "for i in range(3):\n",
    "    print(\"text:\", texts[i])\n",
    "    print(\"tag:\", tags[i])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(\n",
    "    texts, tags, test_size=0.2\n",
    ")\n",
    "\n",
    "# from transformers import DistilBertTokenizerFast\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_texts,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    is_split_into_words=True,\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    val_texts,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    is_split_into_words=True,\n",
    ")\n",
    "\n",
    "\n",
    "unique_tags = set(sorted(tag for doc in tags for tag in doc))\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "unique_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6271386",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83695a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = utils.encode_tags(\n",
    "    [[tag2id[tag] for tag in doc] for doc in train_tags], train_encodings\n",
    ")\n",
    "val_labels = utils.encode_tags(\n",
    "    [[tag2id[tag] for tag in doc] for doc in val_tags], val_encodings\n",
    ")\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\")  # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = utils.WNUTDataset(train_encodings, train_labels)\n",
    "val_dataset = utils.WNUTDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de674278",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### https://huggingface.co/transformers/v3.1.0/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab52331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d147a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(id2tag)\n",
    ")\n",
    "\n",
    "# args that will be logged to wandb\n",
    "logged_training_args = dict(\n",
    "    output_dir=model_output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=256,\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    **logged_training_args,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    new_labels = []\n",
    "    new_predictions = []\n",
    "    for (lbl, pred) in zip(labels, predictions):\n",
    "        new_labels.append([])\n",
    "        new_predictions.append([])\n",
    "        for (l, p) in zip(lbl, pred):\n",
    "            if p != -100 and l != -100:\n",
    "                new_labels[-1].append(id2tag[l])\n",
    "                new_predictions[-1].append(id2tag[p])\n",
    "\n",
    "    # seqeval.f1_score(new_labels, new_predictions)\n",
    "    seqeval_result = seqeval.compute(predictions=new_predictions, references=new_labels)\n",
    "    seqeval_result = {f\"seqeval_{k}\": v for k, v in seqeval_result.items()}\n",
    "    for k in seqeval_result.get(\"MISC\", {}):\n",
    "        seqeval_result[f\"seqeval.MISC.{k}\"] = seqeval_result[\"MISC\"][k]\n",
    "    if \"MISC\" in seqeval_result:\n",
    "        del seqeval_result[\"MISC\"]\n",
    "    for k in seqeval_result.get(\"PER\", {}):\n",
    "        seqeval_result[f\"seqeval.PER.{k}\"] = seqeval_result[\"PER\"][k]\n",
    "    if \"PER\" in seqeval_result:\n",
    "        del seqeval_result[\"PER\"]\n",
    "\n",
    "    return {\n",
    "        # **acc.compute(predictions=predictions.reshape(-1), references=labels.reshape(-1)),\n",
    "        # **f1.compute(predictions=predictions.reshape(-1), references=labels.reshape(-1)),\n",
    "        **seqeval_result\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wandb tensorboard mlflow\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=f\"product-catalog-ner\",\n",
    "    name=os.path.split(model_output_dir.rstrip(\"/\"))[-1],\n",
    "    config={\n",
    "        \"dataset_size\": dataset_size,\n",
    "        \"max_items_per_row\": max_items_per_row,\n",
    "        \"SEPS\": utils.SEPS,\n",
    "        **logged_training_args,\n",
    "    },\n",
    "    # resume=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir '{model_output_dir}'/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8112d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_output = trainer.train(resume_from_checkpoint=False)\n",
    "model.save_pretrained(model_output_dir + \"/latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d553979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_output_dir + \"/latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc568b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_output_dir+'/latest')\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_output_dir+'/latest')\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=model.device)\n",
    "example = utils.preprocess_string(\"81|h3 & 2342v,feds & 32X\")\n",
    "print(\"preprocessed example\", example)\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join subtokens of the same label\n",
    "ner_results_joint = utils.join_subtokens(ner_results)\n",
    "\n",
    "print(\"example\", example)\n",
    "print()\n",
    "for result in ner_results_joint:\n",
    "    # if result[\"entity\"] == \"LABEL_0\":\n",
    "    print(example[result[\"start\"] : result[\"end\"] + 1], result[\"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: actually implement some function to decode this and cleanup the \"##\"\n",
    "# TODO: add special tokens to the tokenizer instead of the separators\n",
    "# TODO: make more realistic data using \"groupby\" ProductNameEn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in utils.join_subtokens(ner_results):\n",
    "    print(r[\"word\"], r[\"score\"], r[\"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052d292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('eda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f9d9e2a173e1cd13c16e2118f2dccd72b335422ee1501aacc513e502b40094"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
